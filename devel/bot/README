bot

Simple 'Bot' Agent for exploration of AIG and related concepts

Notes/Thoughts
--------------

Goals could be expressed as functions maximizing a particular variable
Variables could be described as nodes tagged by a variable identifier, e.g.
	
	(NODE:	Name: "Health"
		Type: VariableNode )
		|
		|
		|
	(LINK:	Type: "VariableValueLink")
		|
		|
		|
	(NODE:	Name: "1.0"
		Type: VariableValueNode)

...or does it make more sense to add a 'data' attribute [QVariant?] to the existing MNode class structure...?



So, an agent could have a set of Goals (expressed as GoalNodes linked to the AgentContext, linked to the SpecificEntity for the Agent...)
The goals each have a QVariant attribute, which contains some sort of structured formula description:
	[ and, max, health ] 
	[ and, min, pain   ]
With that structure, the agent could evaulate the variables in the (3rd) column according to the function perscribed....

It just ocurred to me...goals may not (are not?) binary boolean values - e.g. a goal evaulate to "yes, you've completely fulfilled the goal" or "no, goal unfulfilled" - likely, the goal
will evaulate to somewhere between a TRUE and a FALSE state....so the agent will have to try to maximize all goals, while ranking the goals some how....


-----

Agent structure:

Subsystems - such as movement, bio (e.g. simulate health, energy), visual, speach, hearing,.....

Each subsystem could have its own "loop" or state machine....it would interface with the "outside world" and process in/out of Nodes/Links
Subsys's would have their own implicit goals for which they're trying to maintain homeostasis...
 
----------------

Subsys like movement needs to horzintally query bio sys for energy needs and move
Add interface to subsys for agent to query for actions
Agent try actions to achieve goal and memorize actions tried (MemoryNode/MemoryLink)

-----------------

exception ( var ) - NOTE should we include arg for target value as well?
	construct faux goal to max(var)
	shop for actions to max(var)
	example:
		goal min(hunger)
		act eat
		excep for food
		goal faux max(food)
		act findFood
		learn that
			in future, in excep for food, and then in choose goal, 
			it can first see if any other goals accomplish the food
			...
			or just the first act in faux max(food) can be the findFood act, which reall is the findFood(Goal), ...      ..?
			

-------------------

hungry
	eat
	out of food
		faux(max food)
			
			

P(A|G&C)

Goals
-----
- dynamic
	- can be added and removed during runtime
- goals on same level are siblings
	- if linked from parent as UnorderedLinks, sort by relative STI
	- if linked from parent as OrderedLinks, go to next goal in list (consider: use NextItemLink...?)

- actions can add/remove goals
- agent should find goals from MindSpace instead of storing in a List



Context
-------
- Current Action
- Current Goal
- Current Variables
- History of Actions (to some extent)
- History of Goals (...)
- History of Variables (more important...?)

- Need to compare the similarity of a context to another
- Need to assign a probability that this context will lead to the goal


Thoughts on Learning
--------------------
- A Goal Seeking Agent and its interactions with the Enviornment can be modeled as a Markov decision process (MDP) [ref: http://en.wikipedia.org/wiki/Artificial_neural_network, 'Reinforcement Learning"]
- A Markov decision process is a 4-tuple (S,A,P(),R()) where:
	- S is a finite set of states
	- A is a finite set of actions
	- Pa(s,s') = Pr(s[t+1]=s'|s[t]=s,a[t]=a) is the probability that action 'a' in state 's' at time 't' will lead to state 's'' at time 't+1'
	- Ra(s,s') is the immediate rewared (or expected immediate reward) rec'd after transition to state s' from state s with transition prob Pa(s,s')
- MDP is based on finding a policy function f(s) that the agent will use to choose which action when in state 's'. 
- Goal of f(s) is to maximize some cumultive function of the (random) rewards, given as:
	sum(y[t]R[at](s[t],s[t+1]))
- f(s) can be constructed based on Reinforcement Learning [ref: http://en.wikipedia.org/wiki/Markov_decision_process, 'Reinforcement Learning']
	- f(s) can be given as:
		Q(s,a) = sum(Pa(s,s')(Ra(s,s')+yV(s')))
	- Thus the agent can build up an array of Q(s,a) values giving the outcome (reweard) - to quote wikipedia, "one has an array Q and uses experience to update it directly."
	- This is known as Q-learning
- Q-learning and its relation delayed-Q-learning give the "expected utlity of taking a given action in a given state and following a fixed policy thereafter"
	- ref: http://en.wikipedia.org/wiki/Q-learning
	- Q-learning can be implemented as an ANN

- One problem eventually arises: explore (try something unlearned even if another action has higher probability of success) or exploit current knowledge (use highest probable action of success)
	- This tradeoff has been studied through the "multi-armed bandit" problem
		- Ref: http://en.wikipedia.org/wiki/Multi-armed_bandit
	- Ref: 3rd paragram, http://en.wikipedia.org/wiki/Reinforcement_learning
	- Gittins index can be used to guage when a process is learned:
		http://en.wikipedia.org/wiki/Gittins_index

